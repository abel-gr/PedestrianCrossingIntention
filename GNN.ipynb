{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Graph Neural Network (Spatial GNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of the intention of pedestrians to cross the street or not, using Graph Neural Networks and the coordinates of their skeleton that was previously generated using Openpose in the JAAD dataset.\n",
    "\n",
    "**Input:** Pedestrian skeleton graph.\n",
    "\n",
    "**Output:** Binary classification (crossing or not crossing the street)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch import from_numpy\n",
    "from torch import cuda\n",
    "from torch import no_grad\n",
    "from torch import optim\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from Code.GNN import *\n",
    "from Code.SkeletonsDataset import *\n",
    "from Code.ModelTrainEvaluate import *\n",
    "from Code.MetricsPlots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SkeletonsDataset('Data/train_annotations_with_skeletons.csv', normalization='minmax', target='cross')\n",
    "\n",
    "print('train_dataset len:', len(train_dataset))\n",
    "print('Shape of each skeletons data (x):', train_dataset.data[0].x.shape)\n",
    "\n",
    "train_dataset.loadedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.loadedData[['video','frame','decision_point','skeleton','skeleton_detected','cross','crossing']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of elements per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalRows = len(train_dataset.loadedData)\n",
    "crossingRows = len(train_dataset.loadedData[train_dataset.loadedData['cross']=='crossing'])\n",
    "nocrossingRows = len(train_dataset.loadedData[train_dataset.loadedData['cross']=='not-crossing'])\n",
    "\n",
    "print('Training dataset total rows:', totalRows)\n",
    "print('Training dataset crossing class samples:', crossingRows)\n",
    "print('Training dataset not-crossing class samples:', nocrossingRows)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(1, crossingRows, label='Crossing class')\n",
    "plt.bar(0, nocrossingRows, label='Not-crossing class')\n",
    "plt.legend(loc='upper left', prop={'size': 15})\n",
    "plt.xticks([0, 1], size=15)\n",
    "plt.yticks(size=15)\n",
    "plt.xlabel('Class', size=15)\n",
    "plt.ylabel('Number of samples', size=15)\n",
    "plt.title('Dataset classes distribution', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = train_dataset.data[10].x[:, 0:2].tolist()\n",
    "\n",
    "skeleton2 = []\n",
    "\n",
    "for sk in skeleton:\n",
    "    if(sk!=[0, 0]):\n",
    "        skeleton2.append(sk)\n",
    "        \n",
    "skeleton2 = np.asarray(skeleton2)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(skeleton2[:, 0], skeleton2[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SkeletonsDataset('Data/val_annotations_with_skeletons.csv', normalization='minmax',\n",
    "                               norm_precomputed_values = [train_dataset.xmax, train_dataset.xmin], target='cross')\n",
    "                               # norm_precomputed_values = [train_dataset.xmean, train_dataset.xstd]\n",
    "\n",
    "val_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('val_dataset len:', len(val_dataset))\n",
    "print('Shape of each skeletons data (x):', val_dataset.data[0].x.shape)\n",
    "\n",
    "val_dataset.loadedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.loadedData[['video','frame','decision_point','skeleton','skeleton_detected','cross','crossing']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of elements per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalRows = len(val_dataset.loadedData)\n",
    "crossingRows = len(val_dataset.loadedData[val_dataset.loadedData['cross']=='crossing'])\n",
    "nocrossingRows = len(val_dataset.loadedData[val_dataset.loadedData['cross']=='not-crossing'])\n",
    "\n",
    "print('Validation dataset total rows:', totalRows)\n",
    "print('Validation dataset crossing class samples:', crossingRows)\n",
    "print('Validation dataset not-crossing class samples:', nocrossingRows)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(1, crossingRows, label='Crossing class')\n",
    "plt.bar(0, nocrossingRows, label='Not-crossing class')\n",
    "plt.legend(loc='best', prop={'size': 15})\n",
    "plt.xticks([0, 1], size=15)\n",
    "plt.yticks(size=15)\n",
    "plt.xlabel('Class', size=15)\n",
    "plt.ylabel('Number of samples', size=15)\n",
    "plt.title('Dataset classes distribution', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the classes of the two datasets are unbalanced, we cannot rely only on accuracy as our metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing a skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = val_dataset.data[10].x[:, 0:2].tolist()\n",
    "\n",
    "skeleton2 = []\n",
    "\n",
    "for sk in skeleton:\n",
    "    if(sk!=[0, 0]):\n",
    "        skeleton2.append(sk)\n",
    "        \n",
    "skeleton2 = np.asarray(skeleton2)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(skeleton2[:, 0], skeleton2[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfClasses = 2\n",
    "\n",
    "y = train_dataset.loadedData['cross'].to_numpy()\n",
    "y = np.where(y=='crossing', 1, 0)\n",
    "bc = np.bincount(y)\n",
    "\n",
    "class_weights = len(train_dataset.loadedData) / (numberOfClasses * bc)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print('class_weights:', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shuffle()\n",
    "test_dataset = train_dataset[:5000]\n",
    "train_dataset = train_dataset[5000:]\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First element of training subset:\n",
    "t0 = train_dataset[0]\n",
    "\n",
    "# Node features:\n",
    "t1 = t0.x\n",
    "\n",
    "# Number of nodes:\n",
    "numberOfNodes = t1.shape[0]\n",
    "\n",
    "# Number of dimensions of each node features:\n",
    "embed_dim = t1.shape[1]\n",
    "\n",
    "print('Number of nodes per skeleton:', numberOfNodes)\n",
    "print('Number of features per node:', embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 50\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = SpatialGNN(embed_dim, numberOfClasses, numberOfNodes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "crit = torch.nn.BCELoss()#weight=class_weights)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "metrics_train = []\n",
    "metrics_val = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = train(model, train_loader, device, optimizer, crit)\n",
    "    loss_values.append(train_loss)\n",
    "\n",
    "    train_metrics = evaluate(model, train_loader, device, computed_loss=train_loss)\n",
    "    val_metrics = evaluate(model, val_loader, device, loss_crit=crit)\n",
    "\n",
    "    metrics_train.append(train_metrics)\n",
    "    metrics_val.append(val_metrics)\n",
    "    \n",
    "    if num_epochs <= 25:\n",
    "        \n",
    "        print_evaluation_train_val(epoch, train_metrics, val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(num_epochs, loss_values, figsize=10, textsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_classification_metrics_train_val(num_epochs, metrics_train, metrics_val, figsize=10, textsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc = ROC(model, train_loader, device, numberOfClasses)\n",
    "\n",
    "for plotclass in range(0, numberOfClasses):\n",
    "    plot_ROC(plotclass, fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_auc = ROC(model, val_loader, device, numberOfClasses)\n",
    "\n",
    "for plotclass in range(0, numberOfClasses):\n",
    "    plot_ROC(plotclass, fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_metrics = evaluate(model, test_loader, device)\n",
    "\n",
    "\n",
    "print_evaluation_test(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
